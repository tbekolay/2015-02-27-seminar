{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:48d1bf3847d225886f3f3f5108f3e5aa03b6d3734a2c837fc9216ab492dd14fe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "// change the mode of all current and future CodeMirror instances\n",
      "function to(mode) {\n",
      "    var mode = mode || 'emacs'\n",
      "    // first let's apply emacs mode to all current cells\n",
      "    function to_mode(c) { return c.code_mirror.setOption('keyMap', mode);};\n",
      "    IPython.notebook.get_cells().map(to_mode);\n",
      "    // apply the mode to future cells created\n",
      "    IPython.Cell.options_default.cm_config.keyMap = mode;\n",
      "}\n",
      "\n",
      "require([\"components/codemirror/keymap/emacs\"],\n",
      "     function (emacs) { \n",
      "         to('emacs'); \n",
      "         console.log('emacs.js loaded'); \n",
      "     });"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import nengo\n",
      "from nengo.utils.matplotlib import rasterplot\n",
      "import scipy\n",
      "import scipy.signal\n",
      "from scipy.io.wavfile import read as readwav\n",
      "from IPython.display import Audio"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "# Paper notes\n",
      "\n",
      "Zhang, Heinz, Bruce & Carney 2000:\n",
      "A phenomenological model for the responses of auditory-nerve fibers:\n",
      "1. Nonlinear tuning with compression and suppression\n",
      "\n",
      "- complex sounds: noise-masked stimuli and speech\n",
      "- nonlinear interactions between frequency components in the stimulus\n",
      "  - compressive changes in gain and bandwidth as a function of stimulus level (20 - 110 dB)\n",
      "  - changes in the phase of phase-locked responses\n",
      "  - two-tone suppression\n",
      "- due to cochlear amplifier (Patuzzi & Robertson, 1988; Patuzzi, 1996; Holley, 1996)\n",
      "- i.e., there is a single mechanism for these things\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Building a phenomenological model of the human ear\n",
      "\n",
      "### Computer Science PhD seminar\n",
      "\n",
      "### Trevor Bekolay, Chris Eliasmith"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Motivation\n",
      "\n",
      "A closed-loop biologically plausible auditory system\n",
      "\n",
      "last time: speech synthesis\n",
      "\n",
      "this time: speech sensation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# The human ear\n",
      "\n",
      "translates variations in air pressure to the sounds that we hear and interpret as music, speech, etc.\n",
      "\n",
      "Note the voltage changes and then spiking at the end."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ear physiology\n",
      "\n",
      "Show how the basilar membrane transforms air pressure into frequency components"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Naive? approach\n",
      "\n",
      "Sliding window of time, do FFT,\n",
      "feed that to a spiking neuron.\n",
      "Each frequency recorded = characteristic frequency (CF)\n",
      "of a piece of the membrane, efferent to a cell spiking down the nerve.\n",
      "Each CF is like a very sharply tuned band-pass filter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up audio signals\n",
      "\n",
      "rate = 50000\n",
      "\n",
      "def tone(freq, ramp=0.025, t=0.25, rate=rate):\n",
      "    x = np.linspace(0, t, rate * t)\n",
      "    wave = np.sin(freq * x * 2 * np.pi)\n",
      "    # Ramp up and down\n",
      "    ramp_size = int(rate * ramp)\n",
      "    wave[:ramp_size] *= np.linspace(0, 1, ramp_size)\n",
      "    wave[-ramp_size:] *= np.linspace(1, 0, ramp_size)\n",
      "    return wave\n",
      "\n",
      "def speech(rate=rate, path='energize.wav'):\n",
      "    orig_rate, orig = readwav(path)\n",
      "    new_size = orig.size * (rate / orig_rate)\n",
      "    return scipy.signal.resample(orig, new_size)\n",
      "\n",
      "wave = tone(500)\n",
      "picard = speech()\n",
      "plt.subplot(211)\n",
      "plt.plot(wave)\n",
      "plt.subplot(212)\n",
      "plt.plot(picard)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Audio(data=wave, rate=rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Audio(data=picard, rate=rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.specgram(wave, NFFT=1024, Fs=rate, noverlap=900, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "plt.ylim(top=2000);"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.specgram(picard, NFFT=1024, Fs=rate, noverlap=900, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "plt.ylim(top=6000);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class stfft(object):\n",
      "    def __init__(self, signal, NFFT=1024, rate=rate, minfreq=40, maxfreq=6000):\n",
      "        self.signal = signal\n",
      "        self.NFFT = NFFT\n",
      "        self.rate = rate\n",
      "        self.minfreq = minfreq\n",
      "        self.maxfreq = maxfreq\n",
      "\n",
      "        self._ix = None\n",
      "        self._padded_signal = None\n",
      "\n",
      "    @property\n",
      "    def freqs(self):\n",
      "        allfreqs = np.fft.rfftfreq(self.NFFT, 1. / self.rate)\n",
      "        return allfreqs[(allfreqs >= self.minfreq) & (allfreqs <= self.maxfreq)]\n",
      "\n",
      "    def __call__(self, t):\n",
      "        left = int(t * rate) % self.signal.size\n",
      "\n",
      "        if self._padded_signal is None:\n",
      "            self._padded_signal = np.pad(self.signal, (0, self.NFFT), 'wrap')\n",
      "        if self._ix is None:\n",
      "            allfreqs = np.fft.rfftfreq(self.NFFT, 1. / self.rate)\n",
      "            self._ix = (allfreqs >= self.minfreq) & (allfreqs <= self.maxfreq)\n",
      "\n",
      "        return np.power(np.abs(np.fft.rfft(self._padded_signal[left: left + self.NFFT])[self._ix]), 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "signal = picard\n",
      "weight = 0.00008\n",
      "\n",
      "with nengo.Network() as net:\n",
      "    # Set up a node to output the FFT\n",
      "    fft = stfft(signal=signal)\n",
      "    n_freqs = fft.freqs.size\n",
      "    fft_node = nengo.Node(output=fft, size_out=n_freqs)\n",
      "\n",
      "    # Connect it directly to an ensemble\n",
      "    auditory_nerve = nengo.Ensemble(n_freqs, dimensions=1,\n",
      "                                    intercepts=nengo.dists.Choice([0.5]),\n",
      "                                    max_rates=nengo.dists.Choice([300]))\n",
      "    nengo.Connection(fft_node, auditory_nerve.neurons, transform=weight)\n",
      "    \n",
      "    # Set up probes\n",
      "    fft_probe = nengo.Probe(fft_node, synapse=None)\n",
      "    an_probe = nengo.Probe(auditory_nerve.neurons, synapse=None)\n",
      "\n",
      "sim = nengo.Simulator(net)\n",
      "sim.run(signal.size / rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(sim.trange(), fft.freqs[np.arange(n_freqs)])\n",
      "\n",
      "plt.figure(figsize=(12, 14))\n",
      "ax1 = plt.subplot(311)\n",
      "ax1.pcolormesh(xx, yy, 10 * np.log10(sim.data[fft_probe]).T, cmap=plt.cm.gist_heat)\n",
      "ax1.axis('tight')\n",
      "ax2 = plt.subplot(312)\n",
      "rasterplot(sim.trange(), sim.data[an_probe], ax2)\n",
      "ax2.set_ylim(0, auditory_nerve.n_neurons)\n",
      "ax2.set_ylabel('Neuron')\n",
      "ax2.set_xlabel('Time (s)')\n",
      "ax3 = plt.subplot(313)\n",
      "filtered = nengo.synapses.filtfilt(sim.data[an_probe], nengo.Lowpass(0.005), 0.001)\n",
      "ax3.pcolormesh(xx, yy, filtered.T, cmap=plt.cm.gist_heat)\n",
      "ax3.axis('tight');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# How the ear differs\n",
      "\n",
      "The ear doesn't just do an FFT.\n",
      "\n",
      "## Difference 1: non-uniform sensitivity\n",
      "\n",
      "The inner ear isn't `np.linspace(20, 20000)`.\n",
      "Really it varies per person,\n",
      "but there have been a few scales developed.\n",
      "\n",
      "- http://en.wikipedia.org/wiki/Equivalent_rectangular_bandwidth\n",
      "- http://en.wikipedia.org/wiki/Bark_scale\n",
      "- http://en.wikipedia.org/wiki/Mel_scale"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Less naive approach\n",
      "\n",
      "Filter banks!\n",
      "Allows you to pick any frequencies and be sensitive to them.\n",
      "So pick them from a scale,\n",
      "then design a filter to pick out that frequency."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Designing good filters\n",
      "\n",
      "This is what AN modelers do, all day.\n",
      "\n",
      "Pretty good filter: gammatone.\n",
      "\n",
      "- http://en.wikipedia.org/wiki/Gammatone_filter\n",
      "- http://staffwww.dcs.shef.ac.uk/people/N.Ma/resources/gammatone/\n",
      "- http://www.ee.columbia.edu/ln/rosa/matlab/gammatonegram/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use Brian gammatone filters\n",
      "\n",
      "# http://www.briansimulator.org/docs/hears.html\n",
      "# http://www.briansimulator.org/docs/examples-hears_gammatone.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How the ear differs\n",
      "\n",
      "Guess what: filters aren't independent! Sorry.\n",
      "\n",
      "*   If you double the input to the basilar membrane,\n",
      "    the output less than doubles (saturating / compressing non-linearity).\n",
      "*   If you add a second tone at a different frequency,\n",
      "    the response to the first tone decreases (Two-tone suppression)\n",
      "*   If you play two tones (say 1000 & 1200 Hz)\n",
      "    a third tone can appear (at 800 Hz) - the so-called Cubic Difference Tone.\n",
      "    \n",
      "Nice slides: http://www.lifesci.sussex.ac.uk/home/Chris_Darwin/Perception/Lecture_Notes/Hearing2/hearing2.html\n",
      "\n",
      "the ear's frequency subbands get wider for higher frequencies, whereas the spectrogram has a constant bandwidth across all frequency channels\n",
      "\n",
      "Frequency glide? Maybe ignore?\n",
      "\n",
      "Component 1/Component 2 transition (Kiang 1984, 1990)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Zilany & Bruce phenomenological ear model\n",
      "\n",
      "Here's how Zilany & Bruce modeled the ear non-naively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run the Zilany and Bruce model, either with code or Octave/Matlab magic\n",
      "\n",
      "# http://arokem.github.io/python-matlab-bridge/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Summary\n",
      "\n",
      "The Zilany & Bruce model captures:\n",
      "\n",
      "- Things that made\n",
      "- the ear\n",
      "- different from FFT"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Plans\n",
      "\n",
      "Have the ear and vocal tract interact\n",
      "\n",
      "Use these interactions to improve vocal tract control"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}