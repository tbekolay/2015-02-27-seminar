{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:f2fdf338f19619b6b3d74092917f1ad773898f630a9af114df9e917240000249"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import nengo\n",
      "from nengo.utils.matplotlib import rasterplot\n",
      "import scipy\n",
      "import scipy.signal\n",
      "from scipy.io.wavfile import read as readwav\n",
      "from IPython.display import Audio\n",
      "import brian_no_units\n",
      "import brian\n",
      "import brian.hears"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "%load_ext oct2py.ipython\n",
      "%octave addpath (\"~/Code/auditory_nerve\")\n",
      "\n",
      "def rms(signal):\n",
      "    return np.sqrt(np.mean(signal ** 2))\n",
      "\n",
      "def melspace(low, high, n_freqs):\n",
      "    def hz_to_mel(hz):\n",
      "        return 1127 * np.log(1 + hz / 700.)\n",
      "    def mel_to_hz(mel):\n",
      "        return 700 * (np.exp(mel / 1127.) - 1)\n",
      "    mel_low = hz_to_mel(low)\n",
      "    mel_high = hz_to_mel(high)\n",
      "    mel_x = np.linspace(mel_low, mel_high, n_freqs)\n",
      "    return mel_to_hz(mel_x)\n",
      "\n",
      "sns.set_style(\"white\")\n",
      "sns.set_style(\"ticks\")\n",
      "\n",
      "plt.rcParams['figure.figsize'] = (10.0, 5.0)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Building a <br> phenomenological model of the human ear\n",
      "\n",
      "### Computer Science PhD seminar\n",
      "\n",
      "### Trevor Bekolay, Chris Eliasmith"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Hi, I'm Trevor, and today I'll be walking you through\n",
      "building a model of the human ear.\n",
      "I say that this model is phenomenological\n",
      "because what we're interested in here is\n",
      "the transformation from air pressure waves\n",
      "to spikes travelling down the auditory nerve,\n",
      "rather than building a biophysically realistic\n",
      "model of the ear."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Goal\n",
      "\n",
      "<img src=\"img/spaun.png\" class=\"inline\" width=370>\n",
      "<span class=\"fragment\">\n",
      "  $\\quad \\Rightarrow \\quad$\n",
      "  <img src=\"img/goal.png\" class=\"inline\" width=370>\n",
      "</span>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "As many of you will likely recall,\n",
      "my goal for the next while is to make\n",
      "a closed-loop auditory system.\n",
      "So, to augment systems like Spaun *fragment*\n",
      "with an articulatory speech synthesizer,\n",
      "and an ear model.\n",
      "The eventual goal is to improve\n",
      "the control of the synthesizer\n",
      "by listening to its own generated utterances\n",
      "and comparing those to previous utterances\n",
      "or teaching utterances.\n",
      "\n",
      "Last time, I talked about how the synthesizer works.\n",
      "This time, I'll be talking about how the ear model works."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# The human ear\n",
      "\n",
      "<img src=\"img/an-overview.svg\" class=\"center\" width=\"750\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Let's start with the physiology.\n",
      "How does the human ear work?\n",
      "Apologies to those that have heard this before,\n",
      "but it's always good to review.\n",
      "\n",
      "What we perceive as sound starts at the outer ear\n",
      "as air pressure waves -- vibrating air.\n",
      "That vibrating air causes the\n",
      "Tympanic Membrane, or ear drum,\n",
      "to vibrate, ever so slightly.\n",
      "The ear drum connects to these three tiny bones,\n",
      "known as the ossicles\n",
      "(malleus, incus, and stapes;\n",
      "or, hammer, anvil, and stirrup).\n",
      "These bones amplify and transmit the ear drum vibrations\n",
      "to movements in and out of the inner ear (cochlea).\n",
      "The stapes moves in and out of the oval window,\n",
      "disturbing the liquid inside the cochlea.\n",
      "This causes the basilar membrane in the cochlea to warp,\n",
      "but in a very precise way.\n",
      "As the base of the cochlea, near the start,\n",
      "the membrane will warp when the air pressure wave,\n",
      "and therefore the movement of the stapes,\n",
      "has power at high frequencies,\n",
      "up to about 20000 Hz.\n",
      "Near the apex (end) of the cochlea,\n",
      "the membrane will warp when the incoming wave\n",
      "has power at low frequencies,\n",
      "down to about 25 Hz."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# In the Cochlea\n",
      "\n",
      "<img src=\"img/cochlea.svg\" class=\"inline\" width=\"400\"> $\\quad$\n",
      "<img src=\"img/haircells.svg\" class=\"inline\" width=\"350\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Let's look at the cochlea in more detail.\n",
      "In the cross section at the left,\n",
      "we can see the basilar membrane,\n",
      "as I've already talked about,\n",
      "which is surrounded in liquid\n",
      "that's not really shown here.\n",
      "When the basilar membrane moves\n",
      "in response to the liquid disturbance caused by the stapes,\n",
      "these hair cells get deflected as they\n",
      "are pressed into or pulled from the\n",
      "tectorial membrane, which doesn't move.\n",
      "If you look at the right,\n",
      "if you zoom into these hair cells,\n",
      "they have these protrustions at the top -- which\n",
      "look like hair -- that have ion channels at the top.\n",
      "There is a small attachment from the ion channel\n",
      "to the neighboring protrusion\n",
      "such that deflecting them,\n",
      "as happens when the basilar membrane deflects,\n",
      "opens the ion channel allowing\n",
      "potassium ions to enter the cell.\n",
      "This changes its voltage.\n",
      "\n",
      "Hair cells do not spike;\n",
      "they just change voltage.\n",
      "Each hair cell connects\n",
      "to one or more (I believe)\n",
      "spiral ganglion cells,\n",
      "which do fire action potentials,\n",
      "which travel down the cochlear nerve."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Modelling the auditory periphery\n",
      "\n",
      "$$O(t) \\Rightarrow S_{CF}(t)$$\n",
      "\n",
      "<h3 class=\"fragment\">Naive approach: FFT</h3>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "So how do we go about modelling\n",
      "this complex mechanical device?\n",
      "In our case, what we want\n",
      "is to go from our time-varying audio waveform,\n",
      "$O(t)$, to the spiking activity\n",
      "going down the auditory nerve fibers,\n",
      "which I'll call $S_{CF}(t)$.\n",
      "CF stands for the characteristic frequency,\n",
      "which is the frequency that the\n",
      "piece of the basilar membrane that\n",
      "this neuron is connected to resonates with.\n",
      "*Fragment*\n",
      "The way that we typically pick out frequency components\n",
      "from a signal is the fast fourier transform,\n",
      "so it's not false to say that\n",
      "this is an auditory periphery model.\n",
      "As we'll see, it's overly simplistic,\n",
      "but it gives us a good baseline\n",
      "to see how more sophisticated models differ."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rate = 100000\n",
      "dt = 1. / rate\n",
      "\n",
      "def tone(freq, ramp=0.025, t=0.25, rate=rate):\n",
      "    x = np.linspace(0, t, rate * t)\n",
      "    wave = np.sin(freq * x * 2 * np.pi) * 1.5\n",
      "    # Ramp up and down\n",
      "    ramp_size = int(rate * ramp)\n",
      "    wave[:ramp_size] *= np.linspace(0, 1, ramp_size)\n",
      "    wave[-ramp_size:] *= np.linspace(1, 0, ramp_size)\n",
      "    return wave\n",
      "\n",
      "def whitenoise(t=0.25, rate=rate):\n",
      "    wn = brian.hears.whitenoise(t, samplerate=rate).ramp()\n",
      "    return np.asarray(wn, dtype=np.float64).ravel()\n",
      "\n",
      "def speech(rate=rate, path='speech.wav'):\n",
      "    orig_rate, orig = readwav(path)\n",
      "    new_size = orig.size * (rate / orig_rate)\n",
      "    wave = scipy.signal.resample(orig, new_size)\n",
      "    wave -= wave.mean()\n",
      "    wave *= 0.048\n",
      "    return wave"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "For the code examples that I'll show\n",
      "in the remainder of this presentation,\n",
      "I'll use three different audio signals\n",
      "to test these ear models.\n",
      "One is a tone with a certain frequency,\n",
      "which we use because it should excite\n",
      "a small, predictable number of cells.\n",
      "One is a white noise signal,\n",
      "which we use because it has power\n",
      "in all frequencies, so it should excite all cells.\n",
      "One is a recorded sample of human speech,\n",
      "which we use because it's\n",
      "what we're actually interested in.\n",
      "These are just the functions\n",
      "that create those signals."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tone_500 = tone(500)\n",
      "print('rms=%.3f' % rms(tone_500))\n",
      "plt.plot(np.arange(tone_500.size) * dt, tone_500)\n",
      "plt.ylim(-2, 2)\n",
      "plt.xlabel(\"Time (s)\")\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Audio(data=tone_500, rate=rate)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This is the tone; I a chose 500 Hz arbitrarily.\n",
      "The ramping up and down is something that\n",
      "people tend to do in papers,\n",
      "so I also did it.\n",
      "I tried to keep the RMS of the signals\n",
      "around 1, so that they would be approximately\n",
      "the same volume."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noise = whitenoise()\n",
      "print('rms=%.3f' % rms(noise))\n",
      "plt.plot(np.arange(noise.size) * dt, noise)\n",
      "plt.xlabel(\"Time (s)\")\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Audio(data=noise, rate=rate)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This is the white noise.\n",
      "The ramp up is also something that people tend to do in papers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "human = speech()\n",
      "print('rms=%.3f' % rms(human))\n",
      "plt.plot(np.arange(human.size) * dt, human)\n",
      "plt.xlabel(\"Time (s)\")\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Audio(data=human, rate=rate)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "And this is the human speech sample.\n",
      "From the future!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.specgram(tone_500, NFFT=2048, Fs=rate, noverlap=900, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "plt.ylim(top=2000)\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Here's the spectrogram of the 500 Hz tone.\n",
      "The spectrogram is essentially\n",
      "the Fourier transform over time,\n",
      "so this is mostly a preview of what we\n",
      "are expecting out of auditory model.\n",
      "As you can see,\n",
      "there's high power around 500 Hz\n",
      "and relatively low power elsewhere."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.specgram(noise, NFFT=2048, Fs=rate, noverlap=900, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Here's the spectrogram of the white noise signal.\n",
      "As you can see,\n",
      "there's high power in all frequencies,\n",
      "with some random fluctuations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.specgram(human, NFFT=2048, Fs=rate, noverlap=900, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "plt.ylim(top=6000)\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Finally, the spectrogram of our speech sample.\n",
      "These patterns are very stereotypical of speech.\n",
      "In fact, many people study to be able to read\n",
      "spectrograms of speech;\n",
      "I'm not sure how good they are,\n",
      "but it's possible that some people could look\n",
      "at this and say \"I think they're saying *engage*.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Nengo example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class stfft(object):\n",
      "    def __init__(self, signal, NFFT=2048, rate=rate, minfreq=40, maxfreq=6000):\n",
      "        self.signal = signal\n",
      "        self.NFFT = NFFT\n",
      "        self.rate = rate\n",
      "        self.minfreq = minfreq\n",
      "        self.maxfreq = maxfreq\n",
      "\n",
      "        self._ix = None\n",
      "        self._padded_signal = None\n",
      "\n",
      "    @property\n",
      "    def freqs(self):\n",
      "        allfreqs = np.fft.rfftfreq(self.NFFT, 1. / self.rate)\n",
      "        return allfreqs[(allfreqs >= self.minfreq) & (allfreqs <= self.maxfreq)]\n",
      "\n",
      "    def __call__(self, t):\n",
      "        left = int(t * self.rate) % self.signal.size\n",
      "\n",
      "        if self._padded_signal is None:\n",
      "            self._padded_signal = np.pad(self.signal, (0, self.NFFT), 'wrap')\n",
      "        if self._ix is None:\n",
      "            allfreqs = np.fft.rfftfreq(self.NFFT, 1. / self.rate)\n",
      "            self._ix = (allfreqs >= self.minfreq) & (allfreqs <= self.maxfreq)\n",
      "\n",
      "        return np.power(np.abs(np.fft.rfft(self._padded_signal[left: left + self.NFFT])[self._ix]), 2)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "signal = tone_500\n",
      "weight = 0.001\n",
      "\n",
      "with nengo.Network() as net:\n",
      "    # Set up a node to output the FFT\n",
      "    fft = stfft(signal=signal)\n",
      "    n_freqs = fft.freqs.size\n",
      "    fft_node = nengo.Node(output=fft, size_out=n_freqs)\n",
      "\n",
      "    # Connect it directly to an ensemble\n",
      "    auditory_nerve = nengo.Ensemble(n_freqs, dimensions=1,\n",
      "                                    intercepts=nengo.dists.Choice([0.5]),\n",
      "                                    max_rates=nengo.dists.Choice([300]))\n",
      "    nengo.Connection(fft_node, auditory_nerve.neurons, transform=weight)\n",
      "    \n",
      "    # Set up probes\n",
      "    fft_probe = nengo.Probe(fft_node, synapse=None)\n",
      "    an_probe = nengo.Probe(auditory_nerve.neurons, synapse=None)\n",
      "\n",
      "sim = nengo.Simulator(net)\n",
      "sim.run(signal.size / rate)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(sim.trange(), fft.freqs[np.arange(n_freqs)])\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "ax1 = plt.subplot(311)\n",
      "ax1.pcolormesh(xx, yy, 10 * np.log10(sim.data[fft_probe]).T, cmap=plt.cm.gist_heat)\n",
      "ax1.axis('tight')\n",
      "ax1.set_xticks(())\n",
      "sns.despine(ax=ax1, bottom=True)\n",
      "ax2 = plt.subplot(312)\n",
      "rasterplot(sim.trange(), sim.data[an_probe], ax2)\n",
      "ax2.set_ylim(0, auditory_nerve.n_neurons)\n",
      "ax2.set_ylabel('Neuron')\n",
      "ax2.set_xticks(())\n",
      "sns.despine(ax=ax2, bottom=True)\n",
      "ax3 = plt.subplot(313)\n",
      "filtered = nengo.synapses.filtfilt(sim.data[an_probe], nengo.Lowpass(0.005), sim.dt)\n",
      "ax3.pcolormesh(xx, yy, filtered.T, cmap=plt.cm.gist_heat)\n",
      "ax3.axis('tight')\n",
      "ax3.set_xlabel('Time (s)')\n",
      "sns.despine(ax=ax3)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ear fact 1: logarithmic sensitivity\n",
      "\n",
      "<img src=\"img/bm-scale.svg\" class=\"center\" width=\"500\">\n",
      "\n",
      "- [Bark scale](http://en.wikipedia.org/wiki/Bark_scale)\n",
      "- [Mel scale](http://en.wikipedia.org/wiki/Mel_scale)\n",
      "- [Equivalent Rectangular Bandwidth scale](https://en.wikipedia.org/wiki/Equivalent_rectangular_bandwidth)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "In a sense we've already done what we've set out to do;\n",
      "we've made an ear model!\n",
      "But it's not very faithful to the ear.\n",
      "The reality of the ear is that it doesn't just do an FFT.\n",
      "It is more complex, both for good and bad reasons;\n",
      "it's full of squishy biology stuff,\n",
      "so it can never be as precise as an FFT,\n",
      "but it's also evolved to be incredibly good\n",
      "at listening to natural sounds -- like speech.\n",
      "\n",
      "The first way in which the ear isn't like an FFT\n",
      "is that it cares more about some frequencies than others.\n",
      "This is because of the shape and other mechanical properties\n",
      "of the basical membrane;\n",
      "more surface area resonates with\n",
      "lower frequencies than higher frequencies.\n",
      "People interested in \"psychoacoustics\"\n",
      "discovered this in the early 1900s,\n",
      "and have since formalized this with\n",
      "at least three scales\n",
      "that describe what proportion\n",
      "of the ear is sensitive to what frequencies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mel_cf = melspace(125, 6000, n_freqs)\n",
      "erb_cf = brian.hears.erbspace(125*brian.Hz, 6000*brian.Hz, n_freqs)\n",
      "\n",
      "plt.plot(mel_cf, lw=2, label=\"Mel scale\")\n",
      "plt.plot(erb_cf, lw=2, label=\"ERB scale\")\n",
      "plt.ylabel(\"Frequency (Hz)\")\n",
      "plt.xlabel(\"Neuron\")\n",
      "plt.legend(loc='best')\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Here's what the Mel and ERB scales look like.\n",
      "The Mel scale I implemented myself,\n",
      "and the ERB scale is part of the Brian package.\n",
      "Previously, we had 122 neurons with linearly spaced\n",
      "characteristic frequencies.\n",
      "If we now linearly space along the Mel and ERB scales,\n",
      "we get the following distribution of characteristic frequencies."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Filter banks\n",
      "\n",
      "<img src=\"img/filterbank.jpg\" class=\"center\" width=\"550\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "If we want to be flexible with how we distribute\n",
      "the sensitivities of each neuron,\n",
      "then a better analogy for modelling the ear\n",
      "than an FFT is a filter bank.\n",
      "Each neuron has an associated filter,\n",
      "which attempts to determine when\n",
      "the incoming signal has power\n",
      "in the frequency that it's sensitive to.\n",
      "So, in our model, we'll have a whole bank of filters,\n",
      "one for each characteristic frequency."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Psychoacoustic tuning curves\n",
      "\n",
      "<img src=\"img/tuningcurves.svg\" class=\"center\" width=\"600\">\n",
      "\n",
      "----\n",
      "\n",
      "<small>\n",
      "Zilany, Muhammad S.A., and Ian C. Bruce.\n",
      "Modeling auditory-nerve responses for high sound pressure levels in the normal and impaired auditory periphery.\n",
      "*The Journal of the Acoustical Society of America* 120.3 (2006): 1446-1466.\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "So what's a good filter?\n",
      "Like in other brain areas,\n",
      "we can figure this out\n",
      "by measuring a tuning curve.\n",
      "In psychoacoustics, tuning curves\n",
      "are plotted this way;\n",
      "essentially we're looking at\n",
      "how much the signal is attenuated,\n",
      "so the characteristic frequency\n",
      "is at the bottom."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Decent filter: Gammatone\n",
      "\n",
      "Product of gamma distribution and sine wave.\n",
      "\n",
      "<img src=\"img/gammatone.svg\" class=\"inline\" width=\"400\">\n",
      "<img src=\"img/gammatone-tc.png\" class=\"inline\" width=\"400\">\n",
      "\n",
      "----\n",
      "\n",
      "<small>\n",
      "- Left: [wikipedia](http://en.wikipedia.org/wiki/Gammatone_filter)\n",
      "- Right: [Dan Ellis](http://www.ee.columbia.edu/ln/rosa/matlab/gammatonegram/)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "A nice filter that's been used extensively\n",
      "in auditory modelling is the gammatone filter.\n",
      "It's the product of a gamma distribution\n",
      "-- which, depending on the parameters can look like a PSC curve\n",
      "-- and a sine wave.\n",
      "The sine wave has the frequency you want,\n",
      "and the gamma distribution spreads it out nicely.\n",
      "You end up with \"tuning curves\"\n",
      "that look very similar to the ear.\n",
      "And, it's linear, so it's a nice tradeoff."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sound = brian.hears.Sound(human, samplerate=rate * brian.Hz)\n",
      "gammatone = brian.hears.Gammatone(sound, erb_cf)\n",
      "gt_mon = gammatone.process()\n",
      "\n",
      "xx, yy = np.meshgrid(sound.times, erb_cf)\n",
      "plt.pcolormesh(xx, yy, gt_mon.T, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "if True:\n",
      "    plt.yscale('log')\n",
      "plt.ylabel('Frequency (Hz)')\n",
      "plt.xlabel('Time (s)')\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Brian has gammatone filtering built in.\n",
      "Here's what it looks like filtering the speech signal from before."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Brian + Nengo example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class gammatone(object):\n",
      "    def __init__(self, sound, center_frequencies, rate=rate):\n",
      "        self.t = 0.0\n",
      "        self.rate = rate\n",
      "        self.gtout = brian.hears.Gammatone(sound, center_frequencies).process()\n",
      "\n",
      "    def __call__(self, t):\n",
      "        left = int(self.t * self.rate) % self.gtout.shape[0]\n",
      "        right = int(t * self.rate) % self.gtout.shape[0]\n",
      "        right = self.gtout.shape[0] if right < left else right\n",
      "        self.t = t\n",
      "        data = self.gtout[left: right]\n",
      "        return np.mean(data, axis=0)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sound = brian.hears.Sound(human, samplerate=rate * brian.Hz)\n",
      "weight = 5000\n",
      "\n",
      "with nengo.Network() as net:\n",
      "    # Set up a node to output the Gammatone filter output\n",
      "    gtone = gammatone(sound, erb_cf)\n",
      "    gtone_node = nengo.Node(output=gtone, size_out=n_freqs)\n",
      "\n",
      "    # Connect it directly to an ensemble\n",
      "    auditory_nerve = nengo.Ensemble(n_freqs, dimensions=1,\n",
      "                                    intercepts=nengo.dists.Choice([0.5]),\n",
      "                                    max_rates=nengo.dists.Choice([300]))\n",
      "    nengo.Connection(gtone_node, auditory_nerve.neurons, transform=weight)\n",
      "    \n",
      "    # Set up probes\n",
      "    gtone_probe = nengo.Probe(gtone_node, synapse=None)\n",
      "    an_probe = nengo.Probe(auditory_nerve.neurons, synapse=None)\n",
      "\n",
      "sim = nengo.Simulator(net, dt=0.001)\n",
      "sim.run(sound.duration)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(sim.trange(), erb_cf)\n",
      "\n",
      "logscale = True\n",
      "plt.figure(figsize=(12, 14))\n",
      "ax1 = plt.subplot(311)\n",
      "ax1.pcolormesh(xx, yy, sim.data[gtone_probe].T, cmap=plt.cm.gist_heat)\n",
      "ax1.axis('tight')\n",
      "ax1.set_xticks(())\n",
      "sns.despine(ax=ax1, bottom=True)\n",
      "if logscale:\n",
      "    ax1.set_yscale('log')\n",
      "ax2 = plt.subplot(312)\n",
      "rasterplot(sim.trange(), sim.data[an_probe], ax2)\n",
      "ax2.set_ylim(0, auditory_nerve.n_neurons)\n",
      "ax2.set_ylabel('Neuron')\n",
      "ax2.set_xticks(())\n",
      "sns.despine(ax=ax2, bottom=True)\n",
      "ax3 = plt.subplot(313)\n",
      "filtered = nengo.synapses.filtfilt(sim.data[an_probe], nengo.Lowpass(0.005), sim.dt)\n",
      "ax3.pcolormesh(xx, yy, filtered.T, cmap=plt.cm.gist_heat)\n",
      "ax3.axis('tight')\n",
      "ax3.set_xlabel('Time (s)')\n",
      "if logscale:\n",
      "    ax3.set_yscale('log')\n",
      "sns.despine(ax=ax3)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ear facts 2 - ?: nonlinearities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We've slowly introduced auditory modelling to this point,\n",
      "but now we're going to move super fast\n",
      "because we're getting into esoteric territory.\n",
      "There is much more to ear modeling\n",
      "than figuring out the best linear filter.\n",
      "In fact, there are many nonlinearities\n",
      "that we haven't accounted for yet;\n",
      "it's not clear which are important,\n",
      "but I'll quickly go over three."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "# Ear fact 2: compressing nonlinearity\n",
      "\n",
      "*   If you double the input to the basilar membrane, the output less than doubles."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Ear fact number two.\n",
      "*Read*\n",
      "The ear doesn't respond linearly\n",
      "to increasing volume.\n",
      "In fact, most of the signals going down the ear\n",
      "don't carry that much information about volume\n",
      "(some carry primarily volume information)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ear fact 3: suppressing nonlinearity\n",
      "\n",
      "*   If you add a second tone at a different frequency, the response to the first tone decreases\n",
      "\n",
      "<img src=\"img/twotone.gif\" class=\"center\" width=\"450\">\n",
      "\n",
      "----\n",
      "\n",
      "<small>\n",
      "  From [Chris Darwin](http://www.lifesci.sussex.ac.uk/home/Chris_Darwin/Perception/Lecture_Notes/Hearing2/hearing2.html)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Ear fact number three!\n",
      "*Read*\n",
      "This is sometimes called two-tone suppression."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Cochlear amplifier\n",
      "\n",
      "<img src=\"img/cochlea.svg\" class=\"center\" width=\"500\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Ear facts two and three are linked.\n",
      "It turns out that if the outer hair cells\n",
      "are lesioned, then both the\n",
      "compressing and suppressing nonlinearities disappear.\n",
      "This has led to a hypothesis\n",
      "that the outer hair cells\n",
      "modulate the inner hair cell response.\n",
      "This feedback loop is called the\n",
      "\"cochlear amplifier\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Ear fact 4: peak splitting\n",
      "\n",
      "*   If you increase the amplitude past a certain point,\n",
      "    the phase of the response can shift up to 180 degrees.\n",
      "\n",
      "<img src=\"img/c1c2.png\" class=\"center\" width=\"300\">\n",
      "\n",
      "----\n",
      "\n",
      "<small>\n",
      "  Kiang, Nelson Yuan-sheng. *Curious oddments of auditory-nerve studies.* Hearing research 49.1 (1990): 1-16.\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Our final ear fact is called peak-splitting.\n",
      "The finding here is that *read*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Zilany & Bruce phenomenological ear model\n",
      "\n",
      "<img src=\"img/zilany.svg\" class=\"center\" width=\"890\">\n",
      "\n",
      "----\n",
      "\n",
      "<small>\n",
      "Zilany, Muhammad S.A., and Ian C. Bruce.\n",
      "Modeling auditory-nerve responses for high sound pressure levels in the normal and impaired auditory periphery.\n",
      "*The Journal of the Acoustical Society of America* 120.3 (2006): 1446-1466.\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This has all led up to this model,\n",
      "the Zilany & Bruce model,\n",
      "which captures the nonlinearities we've talked about.\n",
      "\n",
      "The compressing and suppressing nonlinearities\n",
      "are incorporated by the OHC portion of the model.\n",
      "Briefly, the OHC is a very broadly tuned filter.\n",
      "The output of that filter is used to modify\n",
      "the characteristics (bandwidth, sensitivity)\n",
      "of the IHC-related filters.\n",
      "\n",
      "The peak splitting phenomenon is incorporated\n",
      "by using two parallel filters to generated the IHC response.\n",
      "At low amplitudes, the C1 filter is dominant;\n",
      "at high amplitudes, the C2 filter is dominant;\n",
      "then, there is a transition point,\n",
      "just like in the experiment,\n",
      "where both filters contribute to the IHC output\n",
      "and you get the two peaks.\n",
      "\n",
      "There are also some additional parts of this model\n",
      "that we don't have time to talk about, like the middle-ear filter,\n",
      "the synapse model, and the spike generator."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Octave + Nengo example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%octave help ANmodel"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pin = human\n",
      "CFs = erb_cf\n",
      "%octave_push pin\n",
      "%octave_push CFs\n",
      "%octave_push rate"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%octave\n",
      "nrep = 1;  % Only simulate once\n",
      "reptime = size(pin);  % Only simulate once\n",
      "tdres = 1/rate;\n",
      "cohc = 1;  % Normal OHC function\n",
      "cihc = 1;  % Normal IHC function\n",
      "species = 2;  % Human!\n",
      "\n",
      "vihc = zeros(length(CFs), 1e5);  % Not sure why 1e5\n",
      "for i = 1:length(CFs)\n",
      "    vihc(i, :) = model_IHC(pin, CFs(i), nrep, tdres, reptime, cohc, cihc, species);\n",
      "end"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%octave_pull vihc\n",
      "vihc = vihc[:, :pin.size]  # model_IHC seems to get 1e5 samples, minimum"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(np.arange(pin.size) * 1. / rate, erb_cf)\n",
      "\n",
      "logscale = True\n",
      "\n",
      "plt.pcolormesh(xx, yy, vihc, cmap=plt.cm.gist_heat)\n",
      "plt.axis('tight')\n",
      "if True:\n",
      "    plt.yscale('log')\n",
      "plt.ylabel('Frequency (Hz)')\n",
      "plt.xlabel('Time (s)')\n",
      "sns.despine()\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%octave\n",
      "\n",
      "fiberType = \"3\";  % High spontaneous rate\n",
      "noiseType = \"1\";  % Variable noise. Would rather no noise, but not an option.\n",
      "implnt = \"0\";  % Approximate power law adaptation\n",
      "1/0\n",
      "[meanrate, varrate, psth] = model_Synapse(vihc(1, :), CFs(1), nrep, tdres, fiberType, noiseType, implnt)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class zilany(object):\n",
      "    def __init__(self, vihc, rate=rate):\n",
      "        self.t = 0.0\n",
      "        self.rate = rate\n",
      "        self.vihc = vihc.T\n",
      "\n",
      "    def __call__(self, t):\n",
      "        left = int(self.t * self.rate) % self.vihc.shape[0]\n",
      "        right = int(t * self.rate) % self.vihc.shape[0]\n",
      "        right = self.vihc.shape[0] if right < left else right\n",
      "        self.t = t\n",
      "        data = self.vihc[left: right]\n",
      "        return np.mean(data, axis=0)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weight = 80.\n",
      "\n",
      "with nengo.Network() as net:\n",
      "    # Set up a node to output the FFT\n",
      "    z_voltage = zilany(vihc)\n",
      "    z_node = nengo.Node(output=z_voltage, size_out=n_freqs)\n",
      "\n",
      "    # Connect it directly to an ensemble\n",
      "    auditory_nerve = nengo.Ensemble(n_freqs, dimensions=1,\n",
      "                                    intercepts=nengo.dists.Choice([0.5]),\n",
      "                                    max_rates=nengo.dists.Choice([300]))\n",
      "    nengo.Connection(z_node, auditory_nerve.neurons, transform=weight)\n",
      "    \n",
      "    # Set up probes\n",
      "    z_probe = nengo.Probe(z_node, synapse=None)\n",
      "    an_probe = nengo.Probe(auditory_nerve.neurons, synapse=None)\n",
      "\n",
      "sim = nengo.Simulator(net, dt=0.001)\n",
      "sim.run(vihc.shape[1] / rate)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(sim.trange(), erb_cf)\n",
      "\n",
      "logscale = True\n",
      "\n",
      "plt.figure(figsize=(12, 14))\n",
      "ax1 = plt.subplot(311)\n",
      "ax1.pcolormesh(xx, yy, sim.data[z_probe].T, cmap=plt.cm.gist_heat)\n",
      "ax1.axis('tight')\n",
      "ax1.set_xticks(())\n",
      "sns.despine(ax=ax1, bottom=True)\n",
      "if logscale:\n",
      "    ax1.set_yscale('log')\n",
      "ax2 = plt.subplot(312)\n",
      "rasterplot(sim.trange(), sim.data[an_probe], ax2)\n",
      "ax2.set_ylim(0, auditory_nerve.n_neurons)\n",
      "ax2.set_ylabel('Neuron')\n",
      "ax2.set_xticks(())\n",
      "sns.despine(ax=ax2, bottom=True)\n",
      "ax3 = plt.subplot(313)\n",
      "filtered = nengo.synapses.filtfilt(sim.data[an_probe], nengo.Lowpass(0.005), sim.dt)\n",
      "ax3.pcolormesh(xx, yy, filtered.T, cmap=plt.cm.gist_heat)\n",
      "ax3.axis('tight')\n",
      "ax3.set_xlabel('Time (s)')\n",
      "if logscale:\n",
      "    ax3.set_yscale('log')\n",
      "sns.despine(ax=ax3)\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Summary\n",
      "\n",
      "- The ear picks out frequency components in air pressure waves.\n",
      "- Ear models are usually based on filter banks.\n",
      "- Sophisticated models use nonlinear filters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "# Plans\n",
      "\n",
      "- Hook up the Zilany & Bruce model to Nengo more cleanly.\n",
      "- Listen to synthesized sounds.\n",
      "- Learn to improve operational space control."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}